# Exploration vs Exploitation in Pricing Optimization

## Why We Target ~50% Conversion: The Cooperative Game Theory Rationale

Before diving into exploration-exploitation mechanics, it's important to understand
**why** the optimization target is ~50% conversion — not maximum conversion.

In drug auctions, every participant (Hippo, GoodRx, RxSense, etc.) bids for the same
fill. Whoever bids lowest wins. If Hippo wins 100% of fills at a given spread, it means
we're pricing dramatically below competitors. That triggers a **price war**: competitors
lower their bids to recapture volume, we lower further, until everyone's margins are
eroded. This is the **race to the bottom**.

The cooperative game theory insight is the opposite: by deliberately targeting ~50%
conversion (not 100%), Hippo **signals restraint**. We're saying "we'll take half the
market at this price, not all of it." Competitors observe this and have no incentive to
undercut further — they're already winning ~50% of fills at current prices. The market
stabilizes at a higher-price equilibrium where **all participants earn better margins**.
This is a **race to the top**.

The `projected_score()` formula encodes this rationale:

```python
def projected_score(spread):
    conv = max(30, spread - 5)  # equilibrium target conversion, not max conversion
    return spread * conv
```

At 60% spread, the target is ~55% conversion → score 3300. At 30% spread, the target
is ~30% conversion (the minimum viable threshold the business accepts) → score 900.
This correctly reflects the **equilibrium profit landscape**: deep discounts (low spread)
are less attractive not just because the margin is low, but because we deliberately limit
how aggressively we pursue them to avoid triggering competitor retaliation.

**The 30% conversion floor is a business constraint**, not a modeling assumption. Below
30% conversion at any spread, the drug is not commercially viable for the program.

---

## The Problem We're Solving

Every optimization round, the classifier decides: **what spread range should
each drug use?** This is fundamentally an **exploration-exploitation tradeoff**
— a well-studied problem in decision theory known as the **multi-armed bandit**.

### The Bandit Analogy

Imagine a row of slot machines (bandits), each with a different payout rate.
You have limited pulls (time/lookups). Your goal: maximize total payout.

- **Exploration**: try new machines to learn their payout rates
- **Exploitation**: keep pulling the machine you know pays well
- **The tension**: every pull spent exploring is a pull NOT spent on your best known option

In our pricing context:

| Bandit concept | Pricing equivalent |
|---|---|
| Slot machine | A spread range (e.g., 30-60%) |
| Pull the lever | A lookup (API call) at a spread in that range |
| Payout | Fill attempt (conversion) × spread = profit score |
| Unknown payout rate | Conversion rate at untested spreads |

### Why This Matters for Us

The stateless classifier is like someone who walks into the casino with
**amnesia every day**. They don't remember which machines they tried yesterday,
which ones paid zero, or which one was on a hot streak. So they waste pulls
re-testing machines that already proved dead.

Concretely: if Round 3 tested spreads 10-40% and got 0% conversion everywhere,
a stateless classifier in Round 4 might recommend SHIFT LOWER back into
10-40% — because it has no memory of that failure.

## The Three Classic Strategies

### 1. Epsilon-Greedy (simplest)

**Exploit most of the time, explore randomly ε% of the time.**

- Spend 90% of lookups on the best-known spread range
- Spend 10% on random spreads to discover if anything changed

*Pros*: simple, always gets some signal everywhere
*Cons*: wastes exploration budget on obviously bad options, doesn't learn faster

### 2. Upper Confidence Bound (UCB)

**Be optimistic about uncertainty. Try the option where the upside is highest.**

For each spread range, maintain:
- Estimated conversion rate (from observed data)
- Confidence interval (wider = less data = more uncertainty)

Always pick the range with the highest **upper confidence bound** =
estimated rate + uncertainty bonus.

*Pros*: naturally explores high-uncertainty areas, converges faster
*Cons*: needs a mathematical model of uncertainty

### 3. Thompson Sampling (Bayesian)

**Sample from your belief about each option, pick the one that looks best.**

For each spread range, maintain a probability distribution (Beta distribution)
of the conversion rate. Each round:
1. Sample a random conversion rate from each distribution
2. Pick the range whose sample is highest
3. Observe the actual conversion, update the distribution

*Pros*: optimal exploration-exploitation balance, handles regime changes
naturally, widely used in A/B testing platforms
*Cons*: requires Bayesian modeling

## Our Approach: Dual-Window with History

We don't use any of the above directly — our setup has constraints that
classic bandits don't:

1. **We don't control individual lookups.** The POS system randomly assigns
   spreads within our configured range. We only control the range boundaries.
2. **Data arrives in bulk, not one observation at a time.** We see daily
   aggregated bucket data, not real-time per-lookup outcomes.
3. **The environment is non-stationary.** PBM bidding behavior changes over
   time (new competitors, seasonal patterns, formulary changes).
4. **We have history in git.** Every prior spread configuration is versioned,
   with timestamps.

### What We Do Instead

Our history-aware classifier implements a **simplified UCB-like strategy**:

1. **Short window** (since last config change): the "exploitation" data.
   Shows how the current spread range is performing RIGHT NOW.

2. **Long window** (includes pre-change data): the "exploration memory."
   Shows what happened when we tested OTHER spread ranges in prior rounds.

3. **Regime detection**: if conversion at overlapping spreads passes a
   **two-proportion z-test** (z > 1.96, 95% confidence), the market changed.
   A 10pp difference with n=20 lookups is statistically indistinguishable from
   noise; a 3pp difference with n=500 lookups is almost certainly real. The
   z-score is reported in CSV comments alongside each detected regime change
   (e.g., "regime shift at 40%: conv 10%→60% (up, z=4.06)"). This is the
   non-stationarity signal that classic bandits handle with techniques like
   "sliding window UCB" or "discounted Thompson Sampling."

4. **Territory memory**: when the classifier recommends SHIFT HIGHER or
   SHIFT LOWER, the history annotation records whether the target territory
   was already tested in a prior round. If the long window shows 0%
   conversion there, this appears as a note in the CSV comment and JSON
   output — it does NOT change the classifier's recommendation. The
   operator reviews these annotations when making deployment decisions.

### The Moving Average Analogy

Think of it like a stock trader using moving averages:

| Window | Trading analogy | What it captures |
|---|---|---|
| Short (post-change) | 5-day moving average | Current behavior under current config |
| Long (all data) | 21-day moving average | Longer-term trend including prior configs |

When the short and long windows **agree** → high confidence, narrow the range.
When they **diverge** → uncertainty, widen exploration or investigate.

## Key Takeaways for the Team

1. **Every lookup is valuable** — it's a "pull" that teaches us about the
   conversion landscape. Wide ranges (SCAN) sacrifice precision for coverage.
   Narrow ranges (NARROW DOWN) sacrifice coverage for precision.

2. **History prevents waste** — the dual-window approach stops us from
   re-testing ranges that already proved dead, unless there's evidence the
   market changed.

3. **Regime changes are real** — PBM bidding behavior shifts. A spread that
   had 0% conversion last month might have 50% today if a new bidder entered.
   The dual-window detects this.

4. **The default range is the baseline "arm"** — when we haven't tested an NDC
   yet, the default 50-80% range is our prior belief about where the optimal
   spread is. Each round updates that belief.

5. **Convergence is the goal** — after enough rounds, each NDC should settle
   on a narrow range near its optimal spread. The classifier's job is to get
   there as fast as possible without overshooting.

## Related References

- `docs/PRICING_OPTIMIZATION_SKILL.md` — End-to-end execution guide. Phase 3 describes
  the history-aware dual-window analysis implementation. Domain Rules 5, 18-19, 24, 25
  cover classification logic and the cooperative game theory target.

---

## Further Reading

- **Multi-Armed Bandits** (Sutton & Barto, Chapter 2):
  The foundational textbook treatment. Free online:
  http://incompleteideas.net/book/the-book-2nd.html

- **A Survey on Practical Applications of Multi-Armed Bandits** (2020):
  Real-world applications including pricing, ad placement, clinical trials.
  https://arxiv.org/abs/1904.10040

- **Thompson Sampling for Dynamic Pricing** (2017):
  Directly applicable to our problem — optimizing prices when demand is
  uncertain and non-stationary.

- **Bandits for E-Commerce Pricing** (Google Research):
  How Google uses bandit algorithms for dynamic pricing experiments.

- **Epsilon-Greedy vs UCB vs Thompson Sampling** (Lilian Weng's blog):
  Excellent visual comparison of the three strategies.
  https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/
